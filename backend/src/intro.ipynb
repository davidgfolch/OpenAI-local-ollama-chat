{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI with LocalAI (fully free)\n",
    "You can use [OpenAI](https://github.com/openai/openai-python) with [LocalAI](https://github.com/go-skynet/LocalAI) (fully free) in your local machine.\n",
    "## Requirements\n",
    "You will need a minimum of 150GB disk space, 16 GB RAM, and a 64-bit processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LocalAI setup (docker)\n",
    "\n",
    "### Docker setup\n",
    "\n",
    "- Install [docker](https://docs.docker.com/engine/install/).\n",
    "- Optional: set docker data folder to use another disk (partition) to avoid system partition collapse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo nano /etc/docker/daemon.json\n",
    "# Add the \"data-root\" & save (NOTE: REPLACE /mnt/sda5 with your HDD partition)       \n",
    "{\n",
    "    \"data-root\": \"/mnt/sda5/docker/data-root\"\n",
    "}\n",
    "# Restart docker service\n",
    "sudo systemctl restart docker.service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a volume for the models (this avoid LocalAI download the models every time we start the docker container)\n",
    "- Run the LocalAI container (first time will install the container & download the models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker volume create localai-models\n",
    "docker run -p 8080:8080 --name local-ai -ti -v localai-models:/build/models localai/localai:latest-aio-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cursor (VSCode AI IDE) with LocalAI\n",
    "~~Just set the local host/port & a fake api_key:~~ DONT WORK WITH OLLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama (local setup)\n",
    "Using local Ollama with installer in linux (as system service).\n",
    "You will need a compatible GPU hardware to make it run reasonably fast.\n",
    "#### Install \n",
    "- Go to the [Ollama](https://ollama.com/download) web site and use the curl + sh installer\n",
    "- Ensure service is working: `sudo service ollama status`\n",
    "- See logs: `sudo journalctl -u ollama -f`\n",
    "  - Checkout is using GPU (not CPU)\n",
    "  - You can also see the used VRAM % (dynamic) in Nvidia Settings application\n",
    "#### Run a model, depending on your hardware (GPU, CPU, etc)\n",
    "In my case I've tested local Ollama with the following hardware:\n",
    "- Intel® Core™ i7-6700HQ CPU @ 2.60GHz × 8\n",
    "- NVIDIA GeForce GTX 960M (2MB)\n",
    "- Ubuntu 22.04.5 LTS\n",
    "Before installing check you have enough space in your HDD.  [Ollama3.1:8b](https://ollama.com/library/llama3.1) weights 4.7GB.\n",
    "\n",
    "If you don't have enough space or want to store models in another disk partition see below: [Change Ollama models store folder](#change-ollama-models-store-folder)\n",
    "```bash\n",
    "  ollama run llama3.1:8b #recommended\n",
    "  #ollama run tinyllama\n",
    "  #ollama run llama2\n",
    "  ```\n",
    "### Troubleshooting\n",
    "[Ollama docs/troubleshooting](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md)\n",
    "#### Change Ollama models store folder\n",
    "Change ollama models folder to avoid system partition saturation.\n",
    "[Models](https://ollama.com/library?sort=popular) are big, so I've changed the location folder for Ollama models:\n",
    "##### Create new folder (and move models)\n",
    "```bash\n",
    "data_folder=/mnt/sda5/ollama \\\n",
    "&& sudo mkdir -p \"${data_folder}\"/models \\\n",
    "&& sudo chown -R ollama:ollama \"${data_folder}\"\n",
    "```\n",
    "If you already have downloaded models, move de folder to the new location\n",
    "```bash\n",
    "mv /usr/share/ollama/.ollama/models/ /mnt/sda5/ollama\n",
    "```\n",
    "##### Set the OLLAMA_MODELS env variable\n",
    "> Didn't work sudo systemctl edit ollama so edited ollama.service directly\n",
    "\n",
    "> Didn't work export OLLAMA_MODELS in ~/.zshrc did not work when restarting ollama\n",
    "Change ollama.service\n",
    "```bash\n",
    "sudo nano /etc/systemd/system/ollama.service\n",
    "# add this line at the end of [Service] block\n",
    "Environment=\"OLLAMA_MODELS=/mnt/sda5/ollama/models\"\n",
    "#save and exit\n",
    "sudo systemctl daemon-reload\n",
    "sudo systemctl restart ollama\n",
    "```\n",
    "\n",
    "#### Ollama don't run with CPU after Linux systems sleep->wake up\n",
    "```bash\n",
    "sudo systemctl stop ollama\n",
    "sudo rmmod nvidia_uvm && sudo modprobe nvidia_uvm\n",
    "sudo systemctl start ollama\n",
    "```\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain -> OpenAI -> LocalAI\n",
    "\n",
    "### Project setup\n",
    "#### Prerequisites\n",
    "\n",
    "[Install](https://docs.anaconda.com/miniconda/#quick-command-line-install) Conda. (See also: [Conda getting started](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html#before-you-start))\n",
    "```bash\n",
    "eval \"$(/home/slks/miniconda3/bin/conda shell.bash hook)\"\n",
    "```\n",
    "\n",
    "#### Install dependencies\n",
    "[Activate environment](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html#installing-packages) to install dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda activate ./.venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# # Create virtual environment  (ALSO with conda)\n",
    "# python -m venv myenv\n",
    "# # Activate virtual environment\n",
    "# source myenv/bin/activate\n",
    "# # Install dependencies\n",
    "# pip install langchain\n",
    "# pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# some conda commands used to setup project\n",
    "conda init zsh\n",
    "conda info --envs\n",
    "conda init langchain \n",
    "conda create --name langchain\n",
    "conda activate langchain\n",
    "conda config --add channels conda-forge\n",
    "conda config --set channel_priority strict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use LocalAI host & port, the api key is required by the OpenAI library only, just put any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='La profesión de ingeniero en Inteligencia Artificial (IA) tiene efectivamente un buen número de oportunidades y potencial para expandirse, debido a varias razones.\\n\\nLas posibilidades en el campo del engenyeo de algoritmis, son:\\n\\n- **Diversidad. Las áreas disciplinarias han sido incorporadas**.\\nEs poco común ver que los ingenieros especializados en inteligencia artificial también podrían abarcarse una variedad de conocimientos o intereses adicionales que les permiten entrar campos como robótica, visión artificial y más.\\n\\nAdemás esto último no será algo nuevo, debido a la cada vez mayor incorporación entre todos los campos.\\n- **Reemplazo humanodigital, por ejemplo en atención al cliente** en los procesos de call-center.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 169, 'prompt_tokens': 26, 'total_tokens': 195}, 'model_name': 'llama3.1:8b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-c5b758b1-9402-4d34-b9af-1f3000b77f07-0', usage_metadata={'input_tokens': 26, 'output_tokens': 169, 'total_tokens': 195})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from host import hostArgs\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(**hostArgs)\n",
    "llm.invoke(\"La profesión de ingeniero de IA, tiene un futuro prometedor?\")\n",
    "# client.chat.completions.create(model=\"gpt-4\",messages=[{\"role\":\"user\",\"content\":\"La profesión de ingeniero de software, tiene un futuro prometedor?\"}])\n",
    "# chatCompletion = client.chat.completions.create(model=\"gpt-4\",messages=[{\"role\":\"user\",\"content\":\"La profesión de ingeniero de software, tiene un futuro prometedor?\"}])\n",
    "# print(chatCompletion.choices[0].message.content)\n",
    "# chatCompletion.create(messages=[{\"role\":\"user\",\"content\":\"Como dices?\"}], chat_id=chatCompletion.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from host import hostArgs\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(**hostArgs)\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Buena pregunta!\\n\\nEn aprendizaje automático, una función de pérdida (loss function) es un parámetro matemátificado utilizado para cuantificar el desempeño de un modelo prevista contra las observaciones reales. Su objetivo principal es minimizar la diferencia entre las predicciones del modelo y los resultados observados.\\n\\nEn otras palabras, la función de pérdida medidas la distancia o disímulo existente entre los salidos previstas por el algoritmos (predictores) y las realdes observadas(pobres data disponibles). Por lo general , estos números se calculan usando datos conexas y entonces usandose para estimir la capacidad correcta del Algoritmos. \\n\\n\\nAlgunos ejemplos de funciones de pérdida comunes son:\\n\\n- **MSE (Mean Squared Error)**: esta función de pérdida es también conocida como cuadrática o mean absolute deviation. Esto es cuando el error medio en las prediccion se calcular como la magnitud media, es decir el promedio en cueros números de predicción.\\n\\nEjesmplo 1\\nLa funcionde perdida de MSE calcula cuádráticotamente los errors entre predictíons recoletidas por algoritmos y dato pobrecias. \\n\\n\\n- **cross entropyy**: Esta función de pérdida es utilizada para resolver problemas donde tienes la respuesta categoría como clasificación.\\nejemplo de usode (classifications)\\n\\n\\n- Por último, la elección de una fonctione perdida afectara significatavlemente la convergencia del aprendie. \\n\\n**La pregunta anterior sobre la forma función de pèrdidas utilizads es muy útil en datos con valores numeríscos. La forma o configuración del modelod de entrenamientos afectdará las perferencia por tanto también la configuración adecuadia funcion pérela de aprendizaje.**\\n\\n¿Deseas agregar más temas?.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 412, 'prompt_tokens': 49, 'total_tokens': 461}, 'model_name': 'llama3.1:8b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-c362da52-1c8f-406d-bd09-fedb4c607c82-0', usage_metadata={'input_tokens': 49, 'output_tokens': 412, 'total_tokens': 461})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from host import hostArgs\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "model = ChatOpenAI(**hostArgs)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Eres un asistente especializado en {ability}.  Puedes responder preguntas sobre {ability}\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "        ])\n",
    "runnable = prompt | model\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(runnable, get_session_history, input_messages_key=\"input\", history_messages_key=\"history\")\n",
    "with_message_history.invoke(\n",
    "    input={\"history\": \"history1\", \"ability\": \"Ciencia de datos\", \"input\": \"Qué es una función de perdida?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}})\n",
    "#           CPU  GPU (GTX960M)\n",
    "#tinyllama  ?:?? 2:56\n",
    "#llama2     2:24 1:34 (llama2 with host model llama3.1)\n",
    "#llama3.1   5:40 2:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Claro!\\n\\nImagina que tienes una caja llena de juguetes que te gustan de diversa naturaleza. Podrías contarlos (número de juguetes) o agruparlos por diferentes categorías como:\\n\\n- Juguetesp por temas. (**Categorizando**): Aquí está el primer paso para comprernder conceptaos\\n- Edad de niños por lo que cada juegos puede llegar a estar muy amplio.\\n- Colores\\n\\n Ahora tienes más información:\\n\\nJuguetesp están organizados en una forma determinada. En las diversas etapas y tipos como pueden: categorizando un modelo de datos.\\n\\n\\nPídelos.\\n\\nImagina, estas etiquetas con distintas variables que se han identificado con lo datos como por ejemplo:\\n\\n\\n- Edad del hijo\\n- Correo y género\\n\\nTienes algo adicional. Imáxim que este un conjunto amplio en la caja .\\n\\n\\nPara esta explicaciones tienes más de esta forma y los diferentes concepto que tenemos el tener al data set.\\n\\n\\n¡Un paso hacia!\\n\\nPara entender cada uno, podemos compararlo a una categoría muy útil y de usos múltiples en ciencia datos.\\n\\nEl siguiente: categorizando con las bases datos\\n\\n\\nUna de  estas basd se debe con categorias de edades, y un modelo bien compundio es de gran usabilidad como verás pronto.\\nLas bases datos tiene varias tipos por lo general.\\n\\n1) tipo ordenado (numerical).\\n   La cuantificación de variables en dos partes con números ordenados\\n\\n\\n    Estan organizados a continuación:\\n\\n   - Tipo cualitativo, donde un sólo valor se tomar por categoría y estos valores no se pueden modificar.\\n \\n- **Tipo Numérica**: Las categorización por las X en ordenado es muy facil.\\n\\n\\n**Las  categorías son una mejor alternativa a la escalología cuando tenemos muchas variables**\\n\\nEs una forma más estructural.\\n\\nLa mejor manera de hacer esto para el futuro o realizar estos datos es dividilos por partes en categorias numericales donde se tienen dos partes.\\n\\n\\n\\nLa **Variable numérica o categorizar el valor con numeros que pueden entrar en alguna relación como una orden  tanto por escalar**.\\n    - Como la edad se está comprendiendo cuando uno ya puede hacerlo mediante variables tipo ordinal.\\n\\n\\nPor otra parte, no todo son juguetes. Debes saber el modelo de tus datos pero, **determina cuento valores son los que tienen variable ordenamiento como tener una base determinadas**\\n.\\nEstá relacionado a la categoriza para agrupalos en ordenados.\\n\\n\\n\\nImagina un modelo muy bien organizado por edades como te estuvimos mostrando:.\\n\\n\\nEstructúrnellos por grupos más o menos con variables ordenadas por la diferencia que tengamos.\\n\\nPor última tenemos una de mayor compleajdad\\n\\nLas cualitativas no son utilizables como numericas que nos proporciona información mas rápida para las necesidades de todos\\n\\n\\n¡Claro!.\\n\\nLa ultima base que necesitas saber en la categorización de bases dados\\n\\n**El uso como datos numérica también tiene un caracteristica determinadas que te ayudan, es si puedes tener más cantidad que el tipo cualitativo para las X en ordenado**\\n.\\n La edad puede ser muy utiles y determinar con una o todas las otras.\\n\\n\\n\\n¡Claro !\\n\\nEsto último fue más fácil.\\n\\n**Categorando tipos de Variables**, **categorizando variable cuanntíficia, agrupamientos no numerico y variables basadas en  el edada son los pasos más utilizados**\\n.\\n\\n¿Cuano tiene mucho datos?\\n\\nTen que realizar categorias.\\n\\n\\nSon la forma base donde se pueden organzar muy facilmente\\n\\nPara resolver lo siguiente por lo mejor categorización o bases de las datos.\\n\\nToma los datos.\\n\\n¡Imagínate hacer una encuesta a una plaza luna \\n\\nO en tu casa\\n\\n\\nTiene muchas X que no conoces\\n\\nLa edad , nombre \\n\\nLas **variables** en escala se pueden ver para cada pregunta pero cuando las tienes varias que puedes resolver\\n\\nEs útil agrupalas.\\n\\n\\n\\nTomas el total de datos por variable agrupados al igual a como hay much más informacion.\\nEs algo fácil y úil.\\n\\nSi la tienen categorizado es muy fácil ordenaldar.\\n\\n\\n\\nCategorizando los  con X para la estructuración como el agrgrupeamiento ya nos proporcionan mayor facilidad es útil tener much cantidad de variables numerizables.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 942, 'prompt_tokens': 1477, 'total_tokens': 2419}, 'model_name': 'llama3.1:8b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-f60bec1e-c1f6-4761-96b9-a4e9677a8748-0', usage_metadata={'input_tokens': 1477, 'output_tokens': 942, 'total_tokens': 2419})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    input={\"history\": \"history1\", \"ability\": \"Ciencia de datos\", \"input\": \"para un ingeniero de software neofito en este campo, explicame más sobre las bases de ciencia de datos y redes neuronales\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "#          CPU   GPU (GTX960M)\n",
    "#tinyllama 11:38\n",
    "#llama2    12:00 3:31 (llama2 with host model llama3.1)\n",
    "#llama3.1  16:00 7:04 (9:29 with firefox opended)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
